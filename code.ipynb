{"cells":[{"cell_type":"code","source":["# File location and type\n\n# kaggle dataset --- https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud\n\nfile_location = \"/FileStore/tables/card_data.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf.repartition(4)\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Loading dataset and repartitioning","showTitle":true,"inputWidgets":{},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a view or table\n\ntemp_table_name = \"card_data_csv\"\n\ndf.createOrReplaceTempView(temp_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd82bb99-1479-4d5c-be10-8c36df0f1d44"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\n/* Query the created temp table in a SQL cell */\n\nselect * from `card_data_csv`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f66379-6f7f-42ec-8e82-d0e0926a1721"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n# To do so, choose your table name and uncomment the bottom line.\n\npermanent_table_name = \"card_data_csv\"\n\n# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db9631f6-bb4a-42ca-8a3c-0d48af932331"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#No of rows and columns\nprint(df.count(),len(df.columns))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0811af75-4d83-43b3-96f3-e9be8d5dd644"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#No of null values in each column\nfrom pyspark.sql.functions import col,isnan, when, count\n\ndf.select([count(when(isnan(c) | col(c).isNull(),c)).alias(c) for c in df.columns]).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98a92963-19fd-40d1-97b1-602a2867f0a2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#describing numerical columns\npd = df.toPandas()\npd.describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74432a31-5c55-4926-a362-b7a241c39534"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#rename fraud column name to label\ndf = df.withColumnRenamed(\"fraud\",\"label\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d21a28b3-2b5d-45dd-9fba-5bc28bf42b23"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#No of fraudulent and non-fradulent records\ndf.groupBy(\"label\").count().display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aadbcb27-23f7-4360-8ec5-68e547daaa2c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\n\nfraud_df = df.filter(expr(\"label = 1\"))\nprint(\"Fraud Count --- \" + str(fraud_df.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Filtering data","showTitle":true,"inputWidgets":{},"nuid":"1bb56e17-3690-4504-9e44-aa765852843a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["no_fraud_df = df.filter(expr(\"label = 0\"))\nprint(\"Not Fraud Count --- \" + str(no_fraud_df.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdd12c24-540d-42e5-87f4-268a4cd54c15"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# sampling unbalanced data to balanced data by selecting 60k records from each fraud and not fraud dataframes\n\n#For fraudulent data, sampling \ncol_names = df.columns\nfraud_sample_list = fraud_df.sample(False,0.8,2321).take(60000)\nfraud_sample_df = spark.createDataFrame(fraud_sample_list, col_names)\nfraud_sample_df.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Sampling","showTitle":true,"inputWidgets":{},"nuid":"e372c755-83b4-48a1-b6f0-b92e88fd8614"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#For fraudulent data, sampling \ncol_names = df.columns\nnot_fraud_sample_list = no_fraud_df.sample(False,0.8,2321).take(60000)\nnot_fraud_sample_df = spark.createDataFrame(not_fraud_sample_list, col_names)\nnot_fraud_sample_df.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64864ae7-45c6-4be6-83b7-a660b2cecae5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Sampled no of each type of records\nprint(fraud_sample_df.count(), not_fraud_sample_df.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b4e9c5a-64e1-4d88-a30a-b3a708013d41"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# union of all samples\ndataframe = fraud_sample_df.union(not_fraud_sample_df)\nprint(dataframe.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a245a76-9eb7-4a0a-90b8-9f53d338e06a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#casting all columns to float\nfrom pyspark.sql.functions import col\n\ncols = dataframe.columns\nfor col_name in cols:\n    dataframe = dataframe.withColumn(col_name,col(col_name).cast('float'))\n\ndataframe.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6717d89-40f2-403a-9dda-72436a8fa6d5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# corelation between all columns\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.linalg import DenseMatrix, Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.functions import *\n\n\nassembler = VectorAssembler(inputCols=dataframe.columns, outputCol=\"features\",handleInvalid='keep')\nassembled_df = assembler.transform(dataframe).select(\"features\")\n\ncorrelation = Correlation.corr(assembled_df,\"features\",\"pearson\").collect()\n\nrows = correlation[0][0].toArray().tolist()\ncorr_df = spark.createDataFrame(rows,dataframe.columns)\n\ncorr_df.display()\n\"\"\"\n    Since all values are less than 0.8 and greater than -0.8, all columns are dependent on each other\n\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45c6abf5-3717-4785-b540-dfa8198adefc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train_df, test_df = dataframe.randomSplit(weights=[0.7,0.3], seed=2321)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Spliting data for training and testing the model ","showTitle":true,"inputWidgets":{},"nuid":"bf78982c-45b1-4164-810d-22efba97aaa4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(train_df.count(), test_df.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20f21160-c80c-4c67-9424-c9880d80dd26"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\ncols_without_label = train_df.columns\ncols_without_label.remove(\"label\")\nassembled_train_df = VectorAssembler(inputCols=cols_without_label, outputCol=\"cols_vector\",handleInvalid='keep').transform(train_df)\nassembled_test_df = VectorAssembler(inputCols=cols_without_label, outputCol=\"cols_vector\",handleInvalid='keep').transform(test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Assembling all column values to a single column of values","showTitle":true,"inputWidgets":{},"nuid":"bc491eef-0b71-41cd-8307-b886d5f56f5a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\n\nscaled_train_df = MinMaxScaler(inputCol=\"cols_vector\",outputCol=\"cols_scaled\").fit(assembled_train_df).transform(assembled_train_df)\nscaled_test_df = MinMaxScaler(inputCol=\"cols_vector\",outputCol=\"cols_scaled\").fit(assembled_test_df).transform(assembled_test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Scaling numerical values in every column to a range","showTitle":true,"inputWidgets":{},"nuid":"e65593be-10b6-499c-943f-cc1b4055096a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["scaled_train_df.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d62c3139-18a9-44f0-a5a0-16c2d76229aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.classification import LinearSVC\n\nlsvc = LinearSVC(featuresCol=\"cols_scaled\", labelCol=\"label\",maxIter=10000)\nmodel = lsvc.fit(scaled_train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Training the model","showTitle":true,"inputWidgets":{},"nuid":"0560ffb6-f9d5-4e09-875f-0a1e6992ed4d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["predictions = model.transform(scaled_test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Testing the model","showTitle":true,"inputWidgets":{},"nuid":"b1017a15-5210-47a3-a6d8-914509d03d29"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom sklearn.metrics import confusion_matrix\n\nevaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\")\nacc = evaluator.evaluate(predictions)\n \nprint(\"Prediction Accuracy: \", acc)\n\ny_pred=predictions.select(\"prediction\").collect()\ny_orig=predictions.select(\"label\").collect()\n\ncm = confusion_matrix(y_orig, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm) \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Accuracy and Confusion matrix","showTitle":true,"inputWidgets":{},"nuid":"3c2c6519-f266-4221-8424-c6113bdb3cd9"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"credit-card-fraud-detection","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2655122687935775}},"nbformat":4,"nbformat_minor":0}
